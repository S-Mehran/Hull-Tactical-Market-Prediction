{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f37acff",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-15T14:22:41.510500Z",
     "iopub.status.busy": "2025-12-15T14:22:41.509904Z",
     "iopub.status.idle": "2025-12-15T14:22:43.189730Z",
     "shell.execute_reply": "2025-12-15T14:22:43.188819Z"
    },
    "papermill": {
     "duration": 1.684958,
     "end_time": "2025-12-15T14:22:43.191222",
     "exception": false,
     "start_time": "2025-12-15T14:22:41.506264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hull-tactical-market-prediction/train.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/test.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_inference_server.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/templates.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/base_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/relay.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/kaggle_evaluation.proto\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/__init__.py\n",
      "/kaggle/input/hull-lstm-final/pytorch/default/1/best_lstm_model.pth\n",
      "/kaggle/input/hull-lstm-final/pytorch/default/2/best_lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a948251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T14:22:43.197377Z",
     "iopub.status.busy": "2025-12-15T14:22:43.197016Z",
     "iopub.status.idle": "2025-12-15T14:23:04.886588Z",
     "shell.execute_reply": "2025-12-15T14:23:04.885832Z"
    },
    "papermill": {
     "duration": 21.694443,
     "end_time": "2025-12-15T14:23:04.888024",
     "exception": false,
     "start_time": "2025-12-15T14:22:43.193581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading artifacts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts loaded successfully. Input dimension: 59\n",
      "Building history: 1/60. Returning neutral allocation (1.0)\n",
      "Building history: 2/60. Returning neutral allocation (1.0)\n",
      "Building history: 3/60. Returning neutral allocation (1.0)\n",
      "Building history: 4/60. Returning neutral allocation (1.0)\n",
      "Building history: 5/60. Returning neutral allocation (1.0)\n",
      "Building history: 6/60. Returning neutral allocation (1.0)\n",
      "Building history: 7/60. Returning neutral allocation (1.0)\n",
      "Building history: 8/60. Returning neutral allocation (1.0)\n",
      "Building history: 9/60. Returning neutral allocation (1.0)\n",
      "Building history: 10/60. Returning neutral allocation (1.0)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fixed Kaggle Submission for Hull Tactical Market Prediction\n",
    "Handles sequential test data with historical context properly\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONSTANTS & CONFIGURATION\n",
    "# ============================================================================\n",
    "SEQUENCE_LENGTH = 60  # Minimum history needed for LSTM predictions\n",
    "INPUT_DIM = 49  # Will be calculated automatically\n",
    "HIDDEN_SIZE_1 = 128\n",
    "HIDDEN_SIZE_2 = 64\n",
    "HIDDEN_SIZE_3 = 32\n",
    "MLP_OUTPUT_DIM = 1\n",
    "DROPOUT = 0.2\n",
    "MIN_INVESTMENT = 0\n",
    "MAX_INVESTMENT = 1.5\n",
    "CONTINUOUS_COLS=40\n",
    "MISSING_COLS=5\n",
    "BINARY_COLS=10\n",
    "LAG_COLS=3\n",
    "FOURIER_COLS=2\n",
    "\n",
    "# Allocation parameters\n",
    "ALLOCATION_LOOKBACK = 20  # Use last N predictions for allocation calculation\n",
    "ALLOCATION_SPAN = 10      # EWM span for smoothing allocations\n",
    "\n",
    "BASE_MODEL_URL = '/kaggle/input/hull-lstm-final/pytorch/default/2'\n",
    "MODEL_URL = os.path.join(BASE_MODEL_URL, 'best_lstm_model.pth')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class LSTM_Architecture(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size_1=128, hidden_size_2=64, \n",
    "                 hidden_size_3=32, mlp_output_dim=1, dropout=0.2):\n",
    "        super(LSTM_Architecture, self).__init__()\n",
    "        \n",
    "        self.input_size = input_dim\n",
    "        self.hidden_sizes = [hidden_size_1, hidden_size_2, hidden_size_3]\n",
    "        self.output_size = mlp_output_dim\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_sizes[0], \n",
    "                             num_layers=1, dropout=0.2, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_sizes[0], hidden_size=self.hidden_sizes[1], \n",
    "                             num_layers=1, dropout=0.2, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=self.hidden_sizes[1], hidden_size=self.hidden_sizes[2], \n",
    "                             num_layers=1, dropout=0.2, batch_first=True)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(self.hidden_sizes[0])\n",
    "        self.layernorm2 = nn.LayerNorm(self.hidden_sizes[1])\n",
    "        self.layernorm3 = nn.LayerNorm(self.hidden_sizes[2])\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.hidden_sizes[-1], 16)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(16, self.output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.layernorm1(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.layernorm2(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.layernorm3(out)\n",
    "        out = self.dropout_layer(out)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.fc2(out)\n",
    "        return out.squeeze(-1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "def simple_allocation(prediction):\n",
    "    \"\"\"\n",
    "    Convert a single prediction to allocation directly.\n",
    "    Used when we don't have enough prediction history for sophisticated methods.\n",
    "    \n",
    "    Args:\n",
    "        prediction: Float, expected market excess return (typically -0.01 to 0.01)\n",
    "    \n",
    "    Returns:\n",
    "        allocation: Float in [MIN_INVESTMENT, MAX_INVESTMENT]\n",
    "    \"\"\"\n",
    "    # Scale prediction to allocation\n",
    "    # Prediction of 0.01 (1% expected return) -> increase allocation\n",
    "    # Prediction of -0.01 (-1% expected return) -> decrease allocation\n",
    "    \n",
    "    # Use tanh to squash to [-1, 1], then map to [MIN_INVESTMENT, MAX_INVESTMENT]\n",
    "    # Multiplier of 100 means ±1% prediction maps to ±tanh(1) = ±0.76\n",
    "    signal = np.tanh(100 * prediction)\n",
    "    \n",
    "    # Map from [-1, 1] to [MIN_INVESTMENT, MAX_INVESTMENT]\n",
    "    # signal = -1 -> MIN_INVESTMENT (very bearish)\n",
    "    # signal = 0 -> midpoint (neutral)\n",
    "    # signal = +1 -> MAX_INVESTMENT (very bullish)\n",
    "    midpoint = (MIN_INVESTMENT + MAX_INVESTMENT) / 2\n",
    "    half_range = (MAX_INVESTMENT - MIN_INVESTMENT) / 2\n",
    "    allocation = midpoint + signal * half_range\n",
    "    \n",
    "    return np.clip(allocation, MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "\n",
    "\n",
    "def create_positions(preds, span=ALLOCATION_SPAN):\n",
    "    \"\"\"\n",
    "    Convert a series of predictions to portfolio positions using z-score normalization.\n",
    "    \n",
    "    Args:\n",
    "        preds: List or array of predictions\n",
    "        span: EWM span for smoothing\n",
    "    \n",
    "    Returns:\n",
    "        Array of allocations in [MIN_INVESTMENT, MAX_INVESTMENT]\n",
    "    \"\"\"\n",
    "    if len(preds) < 2:\n",
    "        # Not enough data for std calculation\n",
    "        return np.array([simple_allocation(preds[0])])\n",
    "    \n",
    "    s = pd.Series(preds)\n",
    "    \n",
    "    # Normalize predictions using z-score\n",
    "    z = (s - s.mean()) / (s.std() + 1e-8)\n",
    "    \n",
    "    # Smooth with exponential weighted moving average\n",
    "    z = z.ewm(span=span, min_periods=1).mean()\n",
    "    \n",
    "    # Apply non-linear transformation\n",
    "    signal = np.tanh(2.5 * z)\n",
    "    \n",
    "    # Map from [-1, 1] to [MIN_INVESTMENT, MAX_INVESTMENT]\n",
    "    midpoint = (MIN_INVESTMENT + MAX_INVESTMENT) / 2\n",
    "    half_range = (MAX_INVESTMENT - MIN_INVESTMENT) / 2\n",
    "    allocation = midpoint + signal * half_range\n",
    "    \n",
    "    # Convert to numpy array for consistent indexing\n",
    "    return np.clip(allocation.values, MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "\n",
    "\n",
    "def add_fourier_terms_for_inference(df, periods, K=3, prefix=\"fourier\"):\n",
    "    \"\"\"Add Fourier terms for time series decomposition\"\"\"\n",
    "    t = df[\"date_id\"].astype(float).values\n",
    "    new_data = {}\n",
    "    for name, P in periods.items():\n",
    "        for k in range(1, K + 1):\n",
    "            new_data[f\"{prefix}_{name}_sin_{k}\"] = np.sin(2 * np.pi * k * t / P)\n",
    "            new_data[f\"{prefix}_{name}_cos_{k}\"] = np.cos(2 * np.pi * k * t / P)\n",
    "    return pd.concat([df, pd.DataFrame(new_data, index=df.index)], axis=1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TRAINING ARTIFACTS\n",
    "# ============================================================================\n",
    "def load_artifacts():\n",
    "    \"\"\"Load all preprocessing objects and models\"\"\"\n",
    "    global model, scaler, scaler_lags, scaler_fouriers, svd, svd_missing\n",
    "    global model_5, model_21, periods, continuous_cols, binary_cols, missing_cols\n",
    "    global added_columns, lag_cols, fourier_cols, buffer, prediction_history\n",
    "    \n",
    "    print(\"Loading artifacts...\")\n",
    "    \n",
    "    # Initialize buffers\n",
    "    buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    prediction_history = deque(maxlen=ALLOCATION_LOOKBACK)\n",
    "    \n",
    "    # Load training data to get column names and train preprocessing\n",
    "    base_url = \"/kaggle/input/hull-tactical-market-prediction\"\n",
    "    df_train = pd.read_csv(os.path.join(base_url, \"train.csv\"))\n",
    "    \n",
    "    # Define added columns (columns that had NaN in training)\n",
    "    added_columns = []\n",
    "    for col in df_train.columns:\n",
    "        if df_train[col].isna().sum() > 0:\n",
    "            added_columns.append(col)\n",
    "    \n",
    "    # Fill missing values\n",
    "    df_train = df_train.ffill().bfill()\n",
    "    \n",
    "    # Define periods for Fourier terms\n",
    "    total_len = len(df_train)\n",
    "    periods = {\n",
    "        \"weekly\": 5,\n",
    "        \"monthly\": 21,\n",
    "        \"yearly\": 252,\n",
    "        \"cycle_1\": total_len,\n",
    "        \"cycle_2\": total_len / 2,\n",
    "        \"cycle_5\": total_len / 5\n",
    "    }\n",
    "    \n",
    "    # Add Fourier terms\n",
    "    df_train = add_fourier_terms_for_inference(df_train, periods=periods, K=3)\n",
    "    \n",
    "    # Calculate rolling volatilities\n",
    "    df_train[\"volatility_5\"] = df_train[\"market_forward_excess_returns\"].rolling(5).std()\n",
    "    df_train[\"volatility_21\"] = df_train[\"market_forward_excess_returns\"].rolling(21).std()\n",
    "    \n",
    "    # Define macro columns\n",
    "    macro_cols = [c for c in df_train.columns if \"cycle\" in c or \"yearly\" in c]\n",
    "    \n",
    "    # Train model_5\n",
    "    mask = ~df_train[\"volatility_5\"].isna() & ~df_train[macro_cols].isna().any(axis=1)\n",
    "    model_5 = LinearRegression()\n",
    "    model_5.fit(df_train.loc[mask, macro_cols], df_train.loc[mask, \"volatility_5\"])\n",
    "    \n",
    "    # Train model_21\n",
    "    mask = ~df_train[\"volatility_21\"].isna() & ~df_train[macro_cols].isna().any(axis=1)\n",
    "    model_21 = LinearRegression()\n",
    "    model_21.fit(df_train.loc[mask, macro_cols], df_train.loc[mask, \"volatility_21\"])\n",
    "    \n",
    "    # Create lagged features\n",
    "    df_train[\"lagged_market_forward_excess_returns\"] = df_train[\"market_forward_excess_returns\"].shift(1)\n",
    "    df_train[\"lagged_forward_returns\"] = df_train[\"forward_returns\"].shift(1)\n",
    "    df_train[\"lagged_risk_free_rate\"] = df_train[\"risk_free_rate\"].shift(1)\n",
    "    df_train = df_train.dropna(subset=['lagged_market_forward_excess_returns'])\n",
    "    \n",
    "    # Add Fourier predictions\n",
    "    df_train[\"vol_fourier_5\"] = model_5.predict(df_train[macro_cols])\n",
    "    df_train[\"vol_fourier_trend\"] = model_21.predict(df_train[macro_cols])\n",
    "    \n",
    "    # Define feature columns\n",
    "    continuous_cols = df_train.filter(regex='^(M|E|I|P|V|S|MOM)').columns.tolist()\n",
    "    continuous_cols = [c for c in continuous_cols if not c.endswith('_missing') and c in df_train.columns]\n",
    "    \n",
    "    binary_cols = [c for c in df_train.columns if c.startswith(\"D\")]\n",
    "    missing_cols = [c + \"_missing\" for c in added_columns]\n",
    "    lag_cols = ['lagged_market_forward_excess_returns', 'lagged_forward_returns', 'lagged_risk_free_rate']\n",
    "    fourier_cols = ['vol_fourier_trend', 'vol_fourier_5']\n",
    "    \n",
    "    # Fit scalers\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_train[continuous_cols])\n",
    "    \n",
    "    scaler_lags = StandardScaler()\n",
    "    scaler_lags.fit(df_train[lag_cols])\n",
    "    \n",
    "    scaler_fouriers = StandardScaler()\n",
    "    scaler_fouriers.fit(df_train[fourier_cols])\n",
    "    \n",
    "    # Fit SVD\n",
    "    X_train_continuous_scaled = scaler.transform(df_train[continuous_cols])\n",
    "    svd = TruncatedSVD(n_components=CONTINUOUS_COLS, algorithm=\"randomized\", random_state=42)\n",
    "    svd.fit(X_train_continuous_scaled)\n",
    "    \n",
    "    # Create missing indicator columns for training\n",
    "    missing_indicators = pd.DataFrame(index=df_train.index)\n",
    "    for col in added_columns:\n",
    "        # Check original unprocessed data for NaN\n",
    "        df_original = pd.read_csv(os.path.join(base_url, \"train.csv\"))\n",
    "        missing_indicators[col + \"_missing\"] = df_original[col].isna().astype(int)\n",
    "    \n",
    "    svd_missing = TruncatedSVD(n_components=MISSING_COLS, algorithm=\"randomized\", random_state=42)\n",
    "    svd_missing.fit(missing_indicators)\n",
    "    \n",
    "    # Calculate actual input dimension\n",
    "    global INPUT_DIM\n",
    "    INPUT_DIM = CONTINUOUS_COLS + LAG_COLS + len(binary_cols) + FOURIER_COLS + MISSING_COLS  # SVD + lags + binary + fourier + SVD_missing\n",
    "    \n",
    "    # Load model\n",
    "    model = LSTM_Architecture(\n",
    "        input_dim=INPUT_DIM,\n",
    "        hidden_size_1=HIDDEN_SIZE_1,\n",
    "        hidden_size_2=HIDDEN_SIZE_2,\n",
    "        hidden_size_3=HIDDEN_SIZE_3,\n",
    "        mlp_output_dim=MLP_OUTPUT_DIM,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "    \n",
    "    state_dict = torch.load(MODEL_URL, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"Artifacts loaded successfully. Input dimension: {INPUT_DIM}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PREDICTION FUNCTION\n",
    "# ============================================================================\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Main prediction function called for each timestep.\n",
    "    \n",
    "    The competition serves data sequentially, including historical context,\n",
    "    so we build up our buffers naturally and make predictions once we have\n",
    "    enough history.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert to pandas\n",
    "        row = test.to_pandas()\n",
    "        \n",
    "        # Step 1: Create missing flags\n",
    "        row_missing_flags = pd.DataFrame(index=row.index)\n",
    "        for col in added_columns:\n",
    "            if col in row.columns:\n",
    "                row_missing_flags[col + \"_missing\"] = row[col].isna().astype(int)\n",
    "            else:\n",
    "                row_missing_flags[col + \"_missing\"] = 0\n",
    "        \n",
    "        # Step 2: Fill NaNs\n",
    "        row_filled = row.fillna(0)\n",
    "        \n",
    "        # Step 3: Add Fourier terms\n",
    "        row_with_fourier = add_fourier_terms_for_inference(row_filled, periods=periods, K=3)\n",
    "        \n",
    "        # Step 4: Get macro columns and predict volatility features\n",
    "        current_cols = row_with_fourier.columns\n",
    "        macro_inputs = [c for c in current_cols if \"cycle\" in c or \"yearly\" in c]\n",
    "        \n",
    "        pred_trend = model_21.predict(row_with_fourier[macro_inputs])[0]\n",
    "        pred_5 = model_5.predict(row_with_fourier[macro_inputs])[0]\n",
    "        \n",
    "        # Step 5: Process continuous features\n",
    "        available_continuous = [c for c in continuous_cols if c in row_filled.columns]\n",
    "        feat_cont = row_filled[available_continuous].reindex(columns=continuous_cols, fill_value=0)\n",
    "        feat_cont_scaled = scaler.transform(feat_cont)\n",
    "        X_svd = pd.DataFrame(\n",
    "            svd.transform(feat_cont_scaled),\n",
    "            index=row.index,\n",
    "            columns=[f\"svd_{i}\" for i in range(CONTINUOUS_COLS)]\n",
    "        )\n",
    "        \n",
    "        # Step 6: Process lagged features\n",
    "        available_lags = [c for c in lag_cols if c in row_filled.columns]\n",
    "        feat_lags = row_filled[available_lags].reindex(columns=lag_cols, fill_value=0)\n",
    "        X_lags_scaled = pd.DataFrame(\n",
    "            scaler_lags.transform(feat_lags),\n",
    "            index=row.index,\n",
    "            columns=lag_cols\n",
    "        )\n",
    "        \n",
    "        # Step 7: Process binary features\n",
    "        available_binary = [c for c in binary_cols if c in row_filled.columns]\n",
    "        X_binary = row_filled[available_binary].reindex(columns=binary_cols, fill_value=0)\n",
    "        \n",
    "        # Step 8: Process Fourier predictions\n",
    "        X_fouriers_raw = pd.DataFrame(\n",
    "            [[pred_trend, pred_5]],\n",
    "            columns=fourier_cols,\n",
    "            index=row.index\n",
    "        )\n",
    "        X_fouriers_scaled = pd.DataFrame(\n",
    "            scaler_fouriers.transform(X_fouriers_raw),\n",
    "            columns=fourier_cols,\n",
    "            index=row.index\n",
    "        )\n",
    "        \n",
    "        # Step 9: Process missing flags\n",
    "        available_missing = [c for c in missing_cols if c in row_missing_flags.columns]\n",
    "        feat_missing = row_missing_flags[available_missing].reindex(columns=missing_cols, fill_value=0)\n",
    "        X_svd_missing = pd.DataFrame(\n",
    "            svd_missing.transform(feat_missing),\n",
    "            index=row.index,\n",
    "            columns=[f\"svd_missing_{i}\" for i in range(MISSING_COLS)]\n",
    "        )\n",
    "        \n",
    "        # Step 10: Concatenate all features\n",
    "        current_feature_df = pd.concat([\n",
    "            X_svd,\n",
    "            X_lags_scaled,\n",
    "            X_binary,\n",
    "            X_fouriers_scaled,\n",
    "            X_svd_missing\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Flatten and add to buffer\n",
    "        current_feature_vector = current_feature_df.values.flatten()\n",
    "        buffer.append(current_feature_vector)\n",
    "        \n",
    "        # # Check if we have enough history for LSTM\n",
    "        if len(buffer) < SEQUENCE_LENGTH:\n",
    "            # Not enough history yet - return neutral allocation\n",
    "            print(f\"Building history: {len(buffer)}/{SEQUENCE_LENGTH}. Returning neutral allocation (1.0)\")\n",
    "            return 1.0\n",
    "        \n",
    "        # Create input tensor for LSTM\n",
    "        x = torch.tensor(np.stack(list(buffer)), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = model(x).item()\n",
    "        \n",
    "        # Store prediction\n",
    "        prediction_history.append(prediction)\n",
    "        \n",
    "        # Calculate allocation based on prediction history\n",
    "        if len(prediction_history) == 1:\n",
    "            # First prediction - use simple allocation\n",
    "            allocation = simple_allocation(prediction)\n",
    "            print(f\"First prediction: {prediction:.6f} -> Allocation: {allocation:.4f}\")\n",
    "        else:\n",
    "            # Use accumulated predictions for sophisticated allocation\n",
    "            allocations = create_positions(list(prediction_history))\n",
    "            allocation = float(allocations[-1])  # Now safe - returns numpy array\n",
    "            print(f\"Prediction: {prediction:.6f} | History: {len(prediction_history)} | Allocation: {allocation:.4f}\")\n",
    "        \n",
    "        # Final safety clip\n",
    "        allocation = float(np.clip(allocation, MIN_INVESTMENT, MAX_INVESTMENT))\n",
    "        \n",
    "        return allocation\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"ERROR in predict(): {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        # Return neutral allocation on error to avoid format issues\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "# Load all artifacts before starting the server\n",
    "load_artifacts()\n",
    "\n",
    "# Initialize inference server\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5808d",
   "metadata": {
    "papermill": {
     "duration": 0.002092,
     "end_time": "2025-12-15T14:23:04.892438",
     "exception": false,
     "start_time": "2025-12-15T14:23:04.890346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14861981,
     "sourceId": 111543,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 534870,
     "modelInstanceId": 520593,
     "sourceId": 686355,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 534870,
     "modelInstanceId": 520593,
     "sourceId": 686408,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.107719,
   "end_time": "2025-12-15T14:23:07.193620",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-15T14:22:37.085901",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
